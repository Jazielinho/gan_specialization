{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_imagenes(image_array, num_images=25, size=(28, 28)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in a uniform grid.\n",
    "    '''\n",
    "    image_plot = image_array.reshape(-1, *size) * 255\n",
    "    image_plot = image_plot.astype(np.uint8)\n",
    "#     plt.figure(figsize=(15, 4.5))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(5, int(num_images) / 5, i + 1)\n",
    "        plt.imshow(image_plot[i].astype(np.uint8), cmap=plt.cm.binary)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_bloque_generador(dim_entrada: int, dim_salida: int) -> tf.keras.Model:\n",
    "    ''' Genera una capa lineal con normalización por bloques '''\n",
    "    entrada = tf.keras.Input(shape=dim_entrada)\n",
    "    capa_lineal = tf.keras.layers.Dense(units=dim_salida)(entrada)\n",
    "    normalizacion = tf.keras.layers.BatchNormalization()(capa_lineal)\n",
    "    salida_relu = tf.keras.activations.relu(normalizacion)\n",
    "    return tf.keras.Model(inputs=entrada, outputs=salida_relu)\n",
    "\n",
    "def obtener_generador(dim_z: int = 10, dim_imagen: int = 784, dim_oculta: int = 128) -> tf.keras.Model:\n",
    "    ''' Generador de imágenes '''\n",
    "    entrada = tf.keras.Input(shape=dim_z)\n",
    "    salida_1 = obtener_bloque_generador(dim_entrada=dim_z, dim_salida=dim_oculta)(entrada)\n",
    "    salida_2 = obtener_bloque_generador(dim_entrada=dim_oculta, dim_salida=dim_oculta*2)(salida_1)\n",
    "    salida_3 = obtener_bloque_generador(dim_entrada=dim_oculta*2, dim_salida=dim_oculta*4)(salida_2)\n",
    "    salida_4 = obtener_bloque_generador(dim_entrada=dim_oculta*4, dim_salida=dim_oculta*8)(salida_3)\n",
    "    salida_lineal = tf.keras.layers.Dense(units=dim_imagen)(salida_4)\n",
    "    salida_sigmoide = tf.keras.activations.sigmoid(salida_lineal)\n",
    "    return tf.keras.Model(inputs=entrada, outputs=salida_sigmoide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # creating Dense layer with units 7*7*256(batch_size) and input_shape of (100,)\n",
    "    model.add(tf.keras.layers.Dense(7*7*256, use_bias=False, input_shape=(64,)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(tf.keras.layers.Reshape((7, 7, 256)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "model (Functional)           (None, 128)               1920      \n",
      "_________________________________________________________________\n",
      "model_1 (Functional)         (None, 256)               34048     \n",
      "_________________________________________________________________\n",
      "model_2 (Functional)         (None, 512)               133632    \n",
      "_________________________________________________________________\n",
      "model_3 (Functional)         (None, 1024)              529408    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "tf.math.sigmoid (TFOpLambda) (None, 784)               0         \n",
      "=================================================================\n",
      "Total params: 1,502,608\n",
      "Trainable params: 1,498,768\n",
      "Non-trainable params: 3,840\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "obtener_generador().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_ruido(numero_muestra: int, dim_z: int) -> tf.random.normal:\n",
    "    ''' Genera vector aleatorios de dimension (numero_muestra, dim_z) '''\n",
    "    return tf.random.normal((numero_muestra, dim_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_bloque_discriminador(dim_entrada: int, dim_salida: int) -> tf.keras.Model:\n",
    "    ''' Genera una capa lineal con normalización por bloques '''\n",
    "    entrada = tf.keras.Input(shape=dim_entrada)\n",
    "    capa_lineal = tf.keras.layers.Dense(units=dim_salida)(entrada)\n",
    "    salida_leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)(capa_lineal)\n",
    "    return tf.keras.Model(inputs=entrada, outputs=salida_leaky_relu)\n",
    "\n",
    "def obtener_discriminador(dim_imagen: int = 784, dim_oculta: int = 128) -> tf.keras.Model:\n",
    "    ''' Generador de imágenes '''\n",
    "    entrada = tf.keras.Input(shape=dim_imagen)\n",
    "    salida_1 = obtener_bloque_discriminador(dim_entrada=dim_imagen, dim_salida=dim_oculta*4)(entrada)\n",
    "    salida_2 = obtener_bloque_discriminador(dim_entrada=dim_oculta*4, dim_salida=dim_oculta*2)(salida_1)\n",
    "    salida_3 = obtener_bloque_discriminador(dim_entrada=dim_oculta*2, dim_salida=dim_oculta)(salida_2)\n",
    "    salida_lineal = tf.keras.layers.Dense(units=1)(salida_3)\n",
    "    salida_sigmoide = tf.keras.activations.sigmoid(salida_lineal)\n",
    "    return tf.keras.Model(inputs=entrada, outputs=salida_sigmoide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "model_5 (Functional)         (None, 512)               403968    \n",
      "_________________________________________________________________\n",
      "model_6 (Functional)         (None, 256)               132352    \n",
      "_________________________________________________________________\n",
      "model_7 (Functional)         (None, 128)               33408     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "tf.math.sigmoid_1 (TFOpLambd (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 569,857\n",
      "Trainable params: 568,065\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "obtener_discriminador().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_perdida_discriminador(generador: tf.keras.Model, discriminador: tf.keras.Model, imagenes_real, num_imagenes, dim_z):\n",
    "    ''' Obtiene perdida del discriminador '''\n",
    "    ruido = obtener_ruido(numero_muestra=num_imagenes, dim_z=dim_z)\n",
    "    imagenes_falsa = generador(ruido)\n",
    "    prediccion_falsa = discriminador(tf.stop_gradient(imagenes_falsa))\n",
    "    target_falsa = tf.zeros_like(prediccion_falsa)\n",
    "#     perdida_falsa = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=target_falsa, logits=prediccion_falsa))\n",
    "    perdida_falsa = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_true=target_falsa, y_pred=prediccion_falsa))\n",
    "    \n",
    "    prediccion_real = discriminador(imagenes_real)\n",
    "    target_real = tf.ones_like(prediccion_real)\n",
    "#     perdida_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=target_real, logits=prediccion_real))\n",
    "    perdida_real = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_true=target_real, y_pred=prediccion_real))\n",
    "\n",
    "    perdida_discriminador = (perdida_falsa + perdida_real) / 2\n",
    "    return perdida_discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_perdida_generador(generador, discriminador, num_imagenes, dim_z):\n",
    "    ruido = obtener_ruido(numero_muestra=num_imagenes, dim_z=dim_z)\n",
    "    imagenes_falsa = generador(ruido)\n",
    "    prediccion_falsa = discriminador(imagenes_falsa)\n",
    "    target_falsa = tf.ones_like(prediccion_falsa)\n",
    "#     perdida_generador = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=target_falsa, logits=prediccion_falsa))\n",
    "    perdida_generador = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_true=target_falsa, y_pred=prediccion_falsa))\n",
    "\n",
    "    return perdida_generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_mnist_data(batch_size=128):\n",
    "    (mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    mnist_x_train = mnist_x_train.reshape(-1, 784)\n",
    "    mnist_x_test = mnist_x_test.reshape(-1, 784)\n",
    "    mnist_data = np.concatenate((mnist_x_train, mnist_x_test), axis=0)\n",
    "    mnist_data = mnist_data / 255.0\n",
    "    mnist_data = mnist_data.reshape(-1, 28, 28, 1)\n",
    "    mnist_target = np.concatenate((mnist_y_train, mnist_y_test), axis=0)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((mnist_data, mnist_target))\n",
    "    dataset = dataset.shuffle(len(mnist_data)).batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = obtener_mnist_data(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generador = obtener_generador(dim_z=64)\n",
    "generador = create_generator()\n",
    "generador_optimizador = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# discriminador = obtener_discriminador()\n",
    "discriminador = create_discriminator()\n",
    "discriminador_optimizador = tf.keras.optimizers.Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_z = 64\n",
    "n_epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████████████████████████████████████████████▏ | 535/547 [11:04<00:15,  1.32s/it]"
     ]
    }
   ],
   "source": [
    "paso_actual = 0\n",
    "media_perdida_generador = 0\n",
    "media_perdida_discriminador = 0\n",
    "\n",
    "# for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "#     for real, _ in dataset:\n",
    "    for real, _ in tqdm.tqdm(dataset):\n",
    "        actual_tamanho_batch = len(real)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            perdida_discriminador = obtener_perdida_discriminador(generador, discriminador, real, actual_tamanho_batch, dim_z)\n",
    "            gradiente_discriminador = tape.gradient(perdida_discriminador, discriminador.trainable_variables)\n",
    "            discriminador_optimizador.apply_gradients(zip(gradiente_discriminador, discriminador.trainable_variables))\n",
    "        \n",
    "        media_perdida_discriminador += perdida_discriminador / actual_tamanho_batch\n",
    "            \n",
    "        with tf.GradientTape() as tape:\n",
    "            perdida_generador = obtener_perdida_generador(generador, discriminador, actual_tamanho_batch, dim_z)\n",
    "            gradiente_generador = tape.gradient(perdida_generador, generador.trainable_variables)\n",
    "            generador_optimizador.apply_gradients(zip(gradiente_generador, generador.trainable_variables))\n",
    "        \n",
    "        media_perdida_generador += perdida_generador / actual_tamanho_batch\n",
    "        \n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch: {epoch}, discriminador: {media_perdida_discriminador}, generador: {media_perdida_generador}\")\n",
    "        \n",
    "        mostrar_imagenes(generador(obtener_ruido(actual_tamanho_batch, dim_z)).numpy(), num_images=25, size=(28, 28))\n",
    "        mostrar_imagenes(real.numpy(), num_images=25, size=(28, 28))\n",
    "    \n",
    "    media_perdida_generador = 0\n",
    "    media_perdida_discriminador = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real.numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generador(obtener_ruido(actual_tamanho_batch, dim_z)).numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
